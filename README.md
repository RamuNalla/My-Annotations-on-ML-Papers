# My-Annotations-on-ML-Papers

Paper-1: Attention is all you Need (Original Transformer Paper)
Paper-2: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding