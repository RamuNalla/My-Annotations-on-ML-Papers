# My-Annotations-on-ML-Papers

Paper-1: Attention is all you Need (Original Transformer Paper)

Paper-2: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Paper-3: ELMo - Deep Contextualized Word Representations